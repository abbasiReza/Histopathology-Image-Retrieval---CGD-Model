{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a6ca910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(feature_vectors, feature_labels, rank,test_img_list, gallery_vectors=None, gallery_labels=None):\n",
    "    num_features = len(feature_labels)\n",
    "    feature_labels = torch.tensor(feature_labels, device=feature_vectors.device)\n",
    "    gallery_vectors = feature_vectors if gallery_vectors is None else gallery_vectors\n",
    "\n",
    "    dist_matrix = torch.cdist(feature_vectors.unsqueeze(0), gallery_vectors.unsqueeze(0)).squeeze(0)\n",
    "    \n",
    "    if gallery_labels is None:\n",
    "        dist_matrix.fill_diagonal_(float('inf'))\n",
    "        gallery_labels = feature_labels\n",
    "    else:\n",
    "        gallery_labels = torch.tensor(gallery_labels, device=feature_vectors.device)\n",
    "\n",
    "    idx = dist_matrix.topk(k=rank[-1], dim=-1, largest=False)[1]\n",
    "    acc_list = []\n",
    "    for r in rank:\n",
    "        correct = (gallery_labels[idx[:, 0:r]] == feature_labels.unsqueeze(dim=-1)).any(dim=-1).float()\n",
    "        acc_list.append((torch.sum(correct) / num_features).item())\n",
    "    return acc_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27e60137",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net, recall_ids):\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        # obtain feature vectors for all data\n",
    "        for key in eval_dict.keys():\n",
    "            eval_dict[key]['features'] = []\n",
    "            imgList=[]\n",
    "            # for inputs, labels in tqdm(eval_dict[key]['data_loader'], desc='processing {} data'.format(key)):\n",
    "            print('processing {} data'.format(key))\n",
    "            for batch_idx, (inputs, labels, imgs) in enumerate(eval_dict[key]['data_loader']):\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "                features, classes = net(inputs)\n",
    "                eval_dict[key]['features'].append(features)\n",
    "                imgList+=imgs\n",
    "            eval_dict[key]['features'] = torch.cat(eval_dict[key]['features'], dim=0)\n",
    "\n",
    "        # compute recall metric\n",
    "        if data_name == 'isc':\n",
    "            acc_list = recall(eval_dict['test']['features'], test_data_set.label_list, recall_ids,imgList,\n",
    "                              eval_dict['gallery']['features'], gallery_data_set.label_list)\n",
    "        else:\n",
    "            acc_list = recall(eval_dict['test']['features'], test_data_set.label_list, recall_ids,imgList)\n",
    "    \n",
    "    desc = 'Test Epoch {}/{} '.format(epoch, num_epochs)\n",
    "    for index, rank_id in enumerate(recall_ids):\n",
    "        desc += 'R@{}:{:.2f}% '.format(rank_id, acc_list[index] * 100)\n",
    "        results['test_recall@{}'.format(rank_id)].append(acc_list[index] * 100)\n",
    "    print(desc)\n",
    "    return acc_list[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64e0361d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Reza\\anaconda3\\lib\\site-packages\\torchvision\\io\\image.py:11: UserWarning: Failed to load image Python extension: Could not find module 'C:\\Users\\Reza\\anaconda3\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "0\n",
      "['s1', 's2', 's3']\n",
      "['s1', 's2', 's3']\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_bn() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "\u001b[91m[WARN] Cannot find rule for <class 'torch.nn.modules.container.Sequential'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
      "\u001b[91m[WARN] Cannot find rule for <class 'resnet.Bottleneck'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
      "\u001b[91m[WARN] Cannot find rule for <class 'model.GlobalDescriptor'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
      "\u001b[91m[WARN] Cannot find rule for <class 'torch.nn.modules.container.ModuleList'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "\u001b[91m[WARN] Cannot find rule for <class 'model.L2Norm'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
      "[INFO] Register count_bn() for <class 'torch.nn.modules.batchnorm.BatchNorm1d'>.\n",
      "\u001b[91m[WARN] Cannot find rule for <class 'model.Model'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
      "# Model Params: 26.66M FLOPs: 10.64G\n",
      "start\n",
      "end\n",
      "0\n",
      "Train Epoch 1/50 - Loss:1.5932 - Acc:25.00%\n",
      "1\n",
      "Train Epoch 1/50 - Loss:1.7367 - Acc:25.00%\n",
      "2\n",
      "Train Epoch 1/50 - Loss:1.6839 - Acc:33.33%\n",
      "3\n",
      "Train Epoch 1/50 - Loss:1.8030 - Acc:34.38%\n",
      "4\n",
      "Train Epoch 1/50 - Loss:1.7885 - Acc:32.50%\n",
      "5\n",
      "Train Epoch 1/50 - Loss:1.7413 - Acc:33.33%\n",
      "6\n",
      "Train Epoch 1/50 - Loss:1.7034 - Acc:32.14%\n",
      "7\n",
      "Train Epoch 1/50 - Loss:1.6959 - Acc:32.81%\n",
      "8\n",
      "Train Epoch 1/50 - Loss:1.7096 - Acc:31.94%\n",
      "9\n",
      "Train Epoch 1/50 - Loss:1.7091 - Acc:31.25%\n",
      "10\n",
      "Train Epoch 1/50 - Loss:1.6809 - Acc:31.82%\n",
      "11\n",
      "Train Epoch 1/50 - Loss:1.6571 - Acc:31.25%\n",
      "12\n",
      "Train Epoch 1/50 - Loss:1.6580 - Acc:30.77%\n",
      "13\n",
      "Train Epoch 1/50 - Loss:1.6518 - Acc:30.36%\n",
      "14\n",
      "Train Epoch 1/50 - Loss:1.6210 - Acc:31.67%\n",
      "15\n",
      "Train Epoch 1/50 - Loss:1.6192 - Acc:32.03%\n",
      "16\n",
      "Train Epoch 1/50 - Loss:1.6059 - Acc:32.35%\n",
      "17\n",
      "Train Epoch 1/50 - Loss:1.5920 - Acc:32.64%\n",
      "18\n",
      "Train Epoch 1/50 - Loss:1.5747 - Acc:34.21%\n",
      "19\n",
      "Train Epoch 1/50 - Loss:1.5507 - Acc:35.00%\n",
      "20\n",
      "Train Epoch 1/50 - Loss:1.5637 - Acc:33.93%\n",
      "21\n",
      "Train Epoch 1/50 - Loss:1.5591 - Acc:34.09%\n",
      "22\n",
      "Train Epoch 1/50 - Loss:1.5566 - Acc:34.24%\n",
      "23\n",
      "Train Epoch 1/50 - Loss:1.5556 - Acc:34.38%\n",
      "24\n",
      "Train Epoch 1/50 - Loss:1.5399 - Acc:35.00%\n",
      "25\n",
      "Train Epoch 1/50 - Loss:1.5423 - Acc:34.62%\n",
      "26\n",
      "Train Epoch 1/50 - Loss:1.5267 - Acc:35.19%\n",
      "27\n",
      "Train Epoch 1/50 - Loss:1.5152 - Acc:35.71%\n",
      "28\n",
      "Train Epoch 1/50 - Loss:1.5147 - Acc:35.34%\n",
      "29\n",
      "Train Epoch 1/50 - Loss:1.5206 - Acc:35.00%\n",
      "30\n",
      "Train Epoch 1/50 - Loss:1.5105 - Acc:35.08%\n",
      "31\n",
      "Train Epoch 1/50 - Loss:1.5065 - Acc:35.55%\n",
      "32\n",
      "Train Epoch 1/50 - Loss:1.4949 - Acc:35.61%\n",
      "33\n",
      "Train Epoch 1/50 - Loss:1.4789 - Acc:36.40%\n",
      "34\n",
      "Train Epoch 1/50 - Loss:1.4724 - Acc:36.07%\n",
      "35\n",
      "Train Epoch 1/50 - Loss:1.4702 - Acc:35.42%\n",
      "36\n",
      "Train Epoch 1/50 - Loss:1.4570 - Acc:36.49%\n",
      "37\n",
      "Train Epoch 1/50 - Loss:1.4524 - Acc:36.51%\n",
      "38\n",
      "Train Epoch 1/50 - Loss:1.4511 - Acc:36.22%\n",
      "39\n",
      "Train Epoch 1/50 - Loss:1.4349 - Acc:37.50%\n",
      "40\n",
      "Train Epoch 1/50 - Loss:1.4297 - Acc:37.80%\n",
      "41\n",
      "Train Epoch 1/50 - Loss:1.4243 - Acc:38.39%\n",
      "42\n",
      "Train Epoch 1/50 - Loss:1.4235 - Acc:38.37%\n",
      "43\n",
      "Train Epoch 1/50 - Loss:1.4218 - Acc:38.35%\n",
      "44\n",
      "Train Epoch 1/50 - Loss:1.4082 - Acc:39.44%\n",
      "45\n",
      "Train Epoch 1/50 - Loss:1.3991 - Acc:40.22%\n",
      "46\n",
      "Train Epoch 1/50 - Loss:1.3861 - Acc:40.96%\n",
      "47\n",
      "Train Epoch 1/50 - Loss:1.3919 - Acc:40.62%\n",
      "48\n",
      "Train Epoch 1/50 - Loss:1.3816 - Acc:41.33%\n",
      "49\n",
      "Train Epoch 1/50 - Loss:1.3881 - Acc:41.25%\n",
      "50\n",
      "Train Epoch 1/50 - Loss:1.3828 - Acc:41.18%\n",
      "51\n",
      "Train Epoch 1/50 - Loss:1.3744 - Acc:41.83%\n",
      "52\n",
      "Train Epoch 1/50 - Loss:1.3720 - Acc:41.75%\n",
      "53\n",
      "Train Epoch 1/50 - Loss:1.3648 - Acc:42.13%\n",
      "54\n",
      "Train Epoch 1/50 - Loss:1.3605 - Acc:42.73%\n",
      "55\n",
      "Train Epoch 1/50 - Loss:1.3538 - Acc:42.86%\n",
      "56\n",
      "Train Epoch 1/50 - Loss:1.3526 - Acc:43.20%\n",
      "57\n",
      "Train Epoch 1/50 - Loss:1.3511 - Acc:43.32%\n",
      "58\n",
      "Train Epoch 1/50 - Loss:1.3463 - Acc:43.43%\n",
      "59\n",
      "Train Epoch 1/50 - Loss:1.3375 - Acc:43.96%\n",
      "60\n",
      "Train Epoch 1/50 - Loss:1.3366 - Acc:44.26%\n",
      "61\n",
      "Train Epoch 1/50 - Loss:1.3331 - Acc:44.56%\n",
      "62\n",
      "Train Epoch 1/50 - Loss:1.3301 - Acc:45.04%\n",
      "63\n",
      "Train Epoch 1/50 - Loss:1.3273 - Acc:45.31%\n",
      "64\n",
      "Train Epoch 1/50 - Loss:1.3259 - Acc:45.19%\n",
      "65\n",
      "Train Epoch 1/50 - Loss:1.3219 - Acc:45.27%\n",
      "66\n",
      "Train Epoch 1/50 - Loss:1.3168 - Acc:45.34%\n",
      "67\n",
      "Train Epoch 1/50 - Loss:1.3164 - Acc:44.85%\n",
      "68\n",
      "Train Epoch 1/50 - Loss:1.3159 - Acc:44.93%\n",
      "69\n",
      "Train Epoch 1/50 - Loss:1.3143 - Acc:44.82%\n",
      "70\n",
      "Train Epoch 1/50 - Loss:1.3116 - Acc:44.54%\n",
      "71\n",
      "Train Epoch 1/50 - Loss:1.3141 - Acc:44.10%\n",
      "72\n",
      "Train Epoch 1/50 - Loss:1.3112 - Acc:44.01%\n",
      "73\n",
      "Train Epoch 1/50 - Loss:1.3109 - Acc:43.92%\n",
      "74\n",
      "Train Epoch 1/50 - Loss:1.3127 - Acc:43.67%\n",
      "75\n",
      "Train Epoch 1/50 - Loss:1.3091 - Acc:43.75%\n",
      "76\n",
      "Train Epoch 1/50 - Loss:1.3099 - Acc:43.51%\n",
      "77\n",
      "Train Epoch 1/50 - Loss:1.3105 - Acc:43.59%\n",
      "78\n",
      "Train Epoch 1/50 - Loss:1.3109 - Acc:43.35%\n",
      "79\n",
      "Train Epoch 1/50 - Loss:1.3079 - Acc:43.44%\n",
      "80\n",
      "Train Epoch 1/50 - Loss:1.3088 - Acc:43.21%\n",
      "81\n",
      "Train Epoch 1/50 - Loss:1.3064 - Acc:43.14%\n",
      "82\n",
      "Train Epoch 1/50 - Loss:1.3056 - Acc:43.07%\n",
      "83\n",
      "Train Epoch 1/50 - Loss:1.3049 - Acc:42.86%\n",
      "84\n",
      "Train Epoch 1/50 - Loss:1.3045 - Acc:42.65%\n",
      "85\n",
      "Train Epoch 1/50 - Loss:1.3055 - Acc:42.59%\n",
      "86\n",
      "Train Epoch 1/50 - Loss:1.3057 - Acc:42.53%\n",
      "87\n",
      "Train Epoch 1/50 - Loss:1.3040 - Acc:42.76%\n",
      "88\n",
      "Train Epoch 1/50 - Loss:1.3006 - Acc:42.98%\n",
      "89\n",
      "Train Epoch 1/50 - Loss:1.2996 - Acc:43.06%\n",
      "90\n",
      "Train Epoch 1/50 - Loss:1.2983 - Acc:42.86%\n",
      "91\n",
      "Train Epoch 1/50 - Loss:1.2957 - Acc:43.07%\n",
      "92\n",
      "Train Epoch 1/50 - Loss:1.2931 - Acc:43.01%\n",
      "93\n",
      "Train Epoch 1/50 - Loss:1.2905 - Acc:43.22%\n",
      "94\n",
      "Train Epoch 1/50 - Loss:1.2897 - Acc:43.29%\n",
      "95\n",
      "Train Epoch 1/50 - Loss:1.2892 - Acc:43.10%\n",
      "96\n",
      "Train Epoch 1/50 - Loss:1.2893 - Acc:42.91%\n",
      "97\n",
      "Train Epoch 1/50 - Loss:1.2865 - Acc:42.98%\n",
      "98\n",
      "Train Epoch 1/50 - Loss:1.2882 - Acc:42.93%\n",
      "99\n",
      "Train Epoch 1/50 - Loss:1.2902 - Acc:42.88%\n",
      "100\n",
      "Train Epoch 1/50 - Loss:1.2925 - Acc:42.70%\n",
      "101\n",
      "Train Epoch 1/50 - Loss:1.2941 - Acc:42.52%\n",
      "102\n",
      "Train Epoch 1/50 - Loss:1.2916 - Acc:42.60%\n",
      "103\n",
      "Train Epoch 1/50 - Loss:1.2913 - Acc:42.55%\n",
      "104\n",
      "Train Epoch 1/50 - Loss:1.2916 - Acc:42.26%\n",
      "105\n",
      "Train Epoch 1/50 - Loss:1.2892 - Acc:42.33%\n",
      "106\n",
      "Train Epoch 1/50 - Loss:1.2866 - Acc:42.52%\n",
      "107\n",
      "Train Epoch 1/50 - Loss:1.2826 - Acc:42.94%\n",
      "108\n",
      "Train Epoch 1/50 - Loss:1.2792 - Acc:43.12%\n",
      "109\n",
      "Train Epoch 1/50 - Loss:1.2799 - Acc:42.84%\n",
      "110\n",
      "Train Epoch 1/50 - Loss:1.2771 - Acc:43.24%\n",
      "111\n",
      "Train Epoch 1/50 - Loss:1.2731 - Acc:43.64%\n",
      "112\n",
      "Train Epoch 1/50 - Loss:1.2766 - Acc:43.36%\n",
      "113\n",
      "Train Epoch 1/50 - Loss:1.2748 - Acc:43.42%\n",
      "114\n",
      "Train Epoch 1/50 - Loss:1.2710 - Acc:43.70%\n",
      "115\n",
      "Train Epoch 1/50 - Loss:1.2725 - Acc:43.64%\n",
      "116\n",
      "Train Epoch 1/50 - Loss:1.2750 - Acc:43.48%\n",
      "117\n",
      "Train Epoch 1/50 - Loss:1.2737 - Acc:43.33%\n",
      "118\n",
      "Train Epoch 1/50 - Loss:1.2726 - Acc:43.38%\n",
      "119\n",
      "Train Epoch 1/50 - Loss:1.2728 - Acc:43.33%\n",
      "120\n",
      "Train Epoch 1/50 - Loss:1.2729 - Acc:43.18%\n",
      "121\n",
      "Train Epoch 1/50 - Loss:1.2717 - Acc:43.14%\n",
      "122\n",
      "Train Epoch 1/50 - Loss:1.2701 - Acc:43.29%\n",
      "123\n",
      "Train Epoch 1/50 - Loss:1.2674 - Acc:43.35%\n",
      "124\n",
      "Train Epoch 1/50 - Loss:1.2661 - Acc:43.50%\n",
      "125\n",
      "Train Epoch 1/50 - Loss:1.2650 - Acc:43.45%\n",
      "126\n",
      "Train Epoch 1/50 - Loss:1.2634 - Acc:43.41%\n",
      "127\n",
      "Train Epoch 1/50 - Loss:1.2616 - Acc:43.36%\n",
      "128\n",
      "Train Epoch 1/50 - Loss:1.2629 - Acc:43.41%\n",
      "129\n",
      "Train Epoch 1/50 - Loss:1.2628 - Acc:43.37%\n",
      "130\n",
      "Train Epoch 1/50 - Loss:1.2619 - Acc:43.42%\n",
      "131\n",
      "Train Epoch 1/50 - Loss:1.2615 - Acc:43.37%\n",
      "132\n",
      "Train Epoch 1/50 - Loss:1.2580 - Acc:43.61%\n",
      "133\n",
      "Train Epoch 1/50 - Loss:1.2569 - Acc:43.75%\n",
      "134\n",
      "Train Epoch 1/50 - Loss:1.2539 - Acc:43.89%\n",
      "135\n",
      "Train Epoch 1/50 - Loss:1.2518 - Acc:44.03%\n",
      "136\n",
      "Train Epoch 1/50 - Loss:1.2524 - Acc:43.89%\n",
      "137\n",
      "Train Epoch 1/50 - Loss:1.2505 - Acc:44.02%\n",
      "138\n",
      "Train Epoch 1/50 - Loss:1.2508 - Acc:43.97%\n",
      "139\n",
      "Train Epoch 1/50 - Loss:1.2503 - Acc:44.11%\n",
      "140\n",
      "Train Epoch 1/50 - Loss:1.2506 - Acc:43.97%\n",
      "141\n",
      "Train Epoch 1/50 - Loss:1.2499 - Acc:44.01%\n",
      "142\n",
      "Train Epoch 1/50 - Loss:1.2495 - Acc:44.06%\n",
      "143\n",
      "Train Epoch 1/50 - Loss:1.2491 - Acc:44.18%\n",
      "144\n",
      "Train Epoch 1/50 - Loss:1.2486 - Acc:44.22%\n",
      "145\n",
      "Train Epoch 1/50 - Loss:1.2477 - Acc:44.18%\n",
      "146\n",
      "Train Epoch 1/50 - Loss:1.2472 - Acc:44.22%\n",
      "147\n",
      "Train Epoch 1/50 - Loss:1.2443 - Acc:44.51%\n",
      "148\n",
      "Train Epoch 1/50 - Loss:1.2425 - Acc:44.71%\n",
      "149\n",
      "Train Epoch 1/50 - Loss:1.2418 - Acc:44.67%\n",
      "150\n",
      "Train Epoch 1/50 - Loss:1.2433 - Acc:44.62%\n",
      "151\n",
      "Train Epoch 1/50 - Loss:1.2415 - Acc:44.82%\n",
      "152\n",
      "Train Epoch 1/50 - Loss:1.2435 - Acc:44.53%\n",
      "153\n",
      "Train Epoch 1/50 - Loss:1.2414 - Acc:44.72%\n",
      "154\n",
      "Train Epoch 1/50 - Loss:1.2390 - Acc:44.92%\n",
      "155\n",
      "Train Epoch 1/50 - Loss:1.2362 - Acc:45.19%\n",
      "156\n",
      "Train Epoch 1/50 - Loss:1.2343 - Acc:45.38%\n",
      "157\n",
      "Train Epoch 1/50 - Loss:1.2326 - Acc:45.41%\n",
      "158\n",
      "Train Epoch 1/50 - Loss:1.2308 - Acc:45.52%\n",
      "159\n",
      "Train Epoch 1/50 - Loss:1.2326 - Acc:45.55%\n",
      "160\n",
      "Train Epoch 1/50 - Loss:1.2304 - Acc:45.73%\n",
      "161\n",
      "Train Epoch 1/50 - Loss:1.2317 - Acc:45.76%\n",
      "162\n",
      "Train Epoch 1/50 - Loss:1.2297 - Acc:45.94%\n",
      "163\n",
      "Train Epoch 1/50 - Loss:1.2319 - Acc:45.73%\n",
      "164\n",
      "Train Epoch 1/50 - Loss:1.2316 - Acc:45.61%\n",
      "165\n",
      "Train Epoch 1/50 - Loss:1.2318 - Acc:45.48%\n",
      "166\n",
      "Train Epoch 1/50 - Loss:1.2321 - Acc:45.36%\n",
      "167\n",
      "Train Epoch 1/50 - Loss:1.2329 - Acc:45.39%\n",
      "168\n",
      "Train Epoch 1/50 - Loss:1.2319 - Acc:45.49%\n",
      "169\n",
      "Train Epoch 1/50 - Loss:1.2319 - Acc:45.37%\n",
      "170\n",
      "Train Epoch 1/50 - Loss:1.2317 - Acc:45.25%\n",
      "171\n",
      "Train Epoch 1/50 - Loss:1.2307 - Acc:45.28%\n",
      "172\n",
      "Train Epoch 1/50 - Loss:1.2305 - Acc:45.23%\n",
      "173\n",
      "Train Epoch 1/50 - Loss:1.2294 - Acc:45.26%\n",
      "174\n",
      "Train Epoch 1/50 - Loss:1.2288 - Acc:45.29%\n",
      "175\n",
      "Train Epoch 1/50 - Loss:1.2294 - Acc:45.24%\n",
      "176\n",
      "Train Epoch 1/50 - Loss:1.2292 - Acc:45.20%\n",
      "177\n",
      "Train Epoch 1/50 - Loss:1.2290 - Acc:45.15%\n",
      "178\n",
      "Train Epoch 1/50 - Loss:1.2300 - Acc:44.97%\n",
      "179\n",
      "Train Epoch 1/50 - Loss:1.2292 - Acc:45.07%\n",
      "180\n",
      "Train Epoch 1/50 - Loss:1.2294 - Acc:45.10%\n",
      "181\n",
      "Train Epoch 1/50 - Loss:1.2290 - Acc:45.12%\n",
      "182\n",
      "Train Epoch 1/50 - Loss:1.2280 - Acc:45.15%\n",
      "183\n",
      "Train Epoch 1/50 - Loss:1.2266 - Acc:45.24%\n",
      "184\n",
      "Train Epoch 1/50 - Loss:1.2248 - Acc:45.41%\n",
      "185\n",
      "Train Epoch 1/50 - Loss:1.2256 - Acc:45.43%\n",
      "186\n",
      "Train Epoch 1/50 - Loss:1.2264 - Acc:45.39%\n",
      "187\n",
      "Train Epoch 1/50 - Loss:1.2294 - Acc:45.21%\n",
      "188\n",
      "Train Epoch 1/50 - Loss:1.2264 - Acc:45.50%\n",
      "189\n",
      "Train Epoch 1/50 - Loss:1.2266 - Acc:45.46%\n",
      "190\n",
      "Train Epoch 1/50 - Loss:1.2282 - Acc:45.42%\n",
      "191\n",
      "Train Epoch 1/50 - Loss:1.2302 - Acc:45.31%\n",
      "192\n",
      "Train Epoch 1/50 - Loss:1.2310 - Acc:45.21%\n",
      "193\n",
      "Train Epoch 1/50 - Loss:1.2301 - Acc:45.36%\n",
      "194\n",
      "Train Epoch 1/50 - Loss:1.2290 - Acc:45.45%\n",
      "195\n",
      "Train Epoch 1/50 - Loss:1.2293 - Acc:45.41%\n",
      "196\n",
      "Train Epoch 1/50 - Loss:1.2285 - Acc:45.37%\n",
      "197\n",
      "Train Epoch 1/50 - Loss:1.2296 - Acc:45.33%\n",
      "198\n",
      "Train Epoch 1/50 - Loss:1.2286 - Acc:45.29%\n",
      "199\n",
      "Train Epoch 1/50 - Loss:1.2295 - Acc:45.12%\n",
      "200\n",
      "Train Epoch 1/50 - Loss:1.2295 - Acc:45.21%\n",
      "201\n",
      "Train Epoch 1/50 - Loss:1.2301 - Acc:45.05%\n",
      "202\n",
      "Train Epoch 1/50 - Loss:1.2308 - Acc:45.01%\n",
      "203\n",
      "Train Epoch 1/50 - Loss:1.2336 - Acc:44.79%\n",
      "204\n",
      "Train Epoch 1/50 - Loss:1.2327 - Acc:44.88%\n",
      "205\n",
      "Train Epoch 1/50 - Loss:1.2318 - Acc:44.96%\n",
      "206\n",
      "Train Epoch 1/50 - Loss:1.2331 - Acc:44.93%\n",
      "207\n",
      "Train Epoch 1/50 - Loss:1.2306 - Acc:45.13%\n",
      "208\n",
      "Train Epoch 1/50 - Loss:1.2317 - Acc:45.16%\n",
      "209\n",
      "Train Epoch 1/50 - Loss:1.2311 - Acc:45.24%\n",
      "210\n",
      "Train Epoch 1/50 - Loss:1.2297 - Acc:45.38%\n",
      "211\n",
      "Train Epoch 1/50 - Loss:1.2273 - Acc:45.58%\n",
      "212\n",
      "Train Epoch 1/50 - Loss:1.2291 - Acc:45.54%\n",
      "213\n",
      "Train Epoch 1/50 - Loss:1.2300 - Acc:45.44%\n",
      "214\n",
      "Train Epoch 1/50 - Loss:1.2314 - Acc:45.35%\n",
      "215\n",
      "Train Epoch 1/50 - Loss:1.2314 - Acc:45.31%\n",
      "216\n",
      "Train Epoch 1/50 - Loss:1.2315 - Acc:45.33%\n",
      "217\n",
      "Train Epoch 1/50 - Loss:1.2303 - Acc:45.47%\n",
      "218\n",
      "Train Epoch 1/50 - Loss:1.2310 - Acc:45.32%\n",
      "219\n",
      "Train Epoch 1/50 - Loss:1.2320 - Acc:45.17%\n",
      "220\n",
      "Train Epoch 1/50 - Loss:1.2313 - Acc:45.19%\n",
      "221\n",
      "Train Epoch 1/50 - Loss:1.2306 - Acc:45.21%\n",
      "222\n",
      "Train Epoch 1/50 - Loss:1.2304 - Acc:45.07%\n",
      "223\n",
      "Train Epoch 1/50 - Loss:1.2311 - Acc:45.03%\n",
      "224\n",
      "Train Epoch 1/50 - Loss:1.2308 - Acc:45.00%\n",
      "225\n",
      "Train Epoch 1/50 - Loss:1.2307 - Acc:44.97%\n",
      "226\n",
      "Train Epoch 1/50 - Loss:1.2291 - Acc:45.10%\n",
      "227\n",
      "Train Epoch 1/50 - Loss:1.2290 - Acc:45.18%\n",
      "228\n",
      "Train Epoch 1/50 - Loss:1.2286 - Acc:45.25%\n",
      "229\n",
      "Train Epoch 1/50 - Loss:1.2278 - Acc:45.27%\n",
      "230\n",
      "Train Epoch 1/50 - Loss:1.2277 - Acc:45.40%\n",
      "231\n",
      "Train Epoch 1/50 - Loss:1.2270 - Acc:45.37%\n",
      "232\n",
      "Train Epoch 1/50 - Loss:1.2280 - Acc:45.28%\n",
      "233\n",
      "Train Epoch 1/50 - Loss:1.2271 - Acc:45.35%\n",
      "234\n",
      "Train Epoch 1/50 - Loss:1.2258 - Acc:45.48%\n",
      "235\n",
      "Train Epoch 1/50 - Loss:1.2267 - Acc:45.50%\n",
      "236\n",
      "Train Epoch 1/50 - Loss:1.2263 - Acc:45.52%\n",
      "237\n",
      "Train Epoch 1/50 - Loss:1.2265 - Acc:45.54%\n",
      "238\n",
      "Train Epoch 1/50 - Loss:1.2250 - Acc:45.61%\n",
      "239\n",
      "Train Epoch 1/50 - Loss:1.2236 - Acc:45.73%\n",
      "240\n",
      "Train Epoch 1/50 - Loss:1.2237 - Acc:45.75%\n",
      "241\n",
      "Train Epoch 1/50 - Loss:1.2235 - Acc:45.76%\n",
      "242\n",
      "Train Epoch 1/50 - Loss:1.2225 - Acc:45.78%\n",
      "243\n",
      "Train Epoch 1/50 - Loss:1.2227 - Acc:45.70%\n",
      "244\n",
      "Train Epoch 1/50 - Loss:1.2220 - Acc:45.77%\n",
      "245\n",
      "Train Epoch 1/50 - Loss:1.2217 - Acc:45.73%\n",
      "246\n",
      "Train Epoch 1/50 - Loss:1.2207 - Acc:45.85%\n",
      "247\n",
      "Train Epoch 1/50 - Loss:1.2198 - Acc:45.87%\n",
      "248\n",
      "Train Epoch 1/50 - Loss:1.2193 - Acc:45.93%\n",
      "249\n",
      "Train Epoch 1/50 - Loss:1.2179 - Acc:46.00%\n",
      "250\n",
      "Train Epoch 1/50 - Loss:1.2169 - Acc:46.12%\n",
      "251\n",
      "Train Epoch 1/50 - Loss:1.2183 - Acc:46.13%\n",
      "252\n",
      "Train Epoch 1/50 - Loss:1.2192 - Acc:46.15%\n",
      "253\n",
      "Train Epoch 1/50 - Loss:1.2185 - Acc:46.16%\n",
      "254\n",
      "Train Epoch 1/50 - Loss:1.2178 - Acc:46.18%\n",
      "255\n",
      "Train Epoch 1/50 - Loss:1.2164 - Acc:46.34%\n",
      "256\n",
      "Train Epoch 1/50 - Loss:1.2152 - Acc:46.40%\n",
      "257\n",
      "Train Epoch 1/50 - Loss:1.2141 - Acc:46.46%\n",
      "258\n",
      "Train Epoch 1/50 - Loss:1.2152 - Acc:46.43%\n",
      "259\n",
      "Train Epoch 1/50 - Loss:1.2131 - Acc:46.63%\n",
      "260\n",
      "Train Epoch 1/50 - Loss:1.2111 - Acc:46.84%\n",
      "261\n",
      "Train Epoch 1/50 - Loss:1.2112 - Acc:46.90%\n",
      "262\n",
      "Train Epoch 1/50 - Loss:1.2090 - Acc:47.10%\n",
      "263\n",
      "Train Epoch 1/50 - Loss:1.2090 - Acc:47.11%\n",
      "264\n",
      "Train Epoch 1/50 - Loss:1.2102 - Acc:47.03%\n",
      "265\n",
      "Train Epoch 1/50 - Loss:1.2111 - Acc:46.90%\n",
      "266\n",
      "Train Epoch 1/50 - Loss:1.2103 - Acc:46.96%\n",
      "267\n",
      "Train Epoch 1/50 - Loss:1.2107 - Acc:46.97%\n",
      "268\n",
      "Train Epoch 1/50 - Loss:1.2104 - Acc:46.93%\n",
      "269\n",
      "Train Epoch 1/50 - Loss:1.2100 - Acc:46.99%\n",
      "270\n",
      "Train Epoch 1/50 - Loss:1.2092 - Acc:47.05%\n",
      "271\n",
      "Train Epoch 1/50 - Loss:1.2092 - Acc:47.10%\n",
      "272\n",
      "Train Epoch 1/50 - Loss:1.2088 - Acc:47.16%\n",
      "273\n",
      "Train Epoch 1/50 - Loss:1.2085 - Acc:47.08%\n",
      "274\n",
      "Train Epoch 1/50 - Loss:1.2084 - Acc:47.05%\n",
      "275\n",
      "Train Epoch 1/50 - Loss:1.2085 - Acc:46.97%\n",
      "276\n",
      "Train Epoch 1/50 - Loss:1.2082 - Acc:46.98%\n",
      "277\n",
      "Train Epoch 1/50 - Loss:1.2076 - Acc:47.08%\n",
      "278\n",
      "Train Epoch 1/50 - Loss:1.2061 - Acc:47.22%\n",
      "279\n",
      "Train Epoch 1/50 - Loss:1.2063 - Acc:47.14%\n",
      "280\n",
      "Train Epoch 1/50 - Loss:1.2053 - Acc:47.20%\n",
      "281\n",
      "Train Epoch 1/50 - Loss:1.2040 - Acc:47.34%\n",
      "282\n",
      "Train Epoch 1/50 - Loss:1.2045 - Acc:47.35%\n",
      "283\n",
      "Train Epoch 1/50 - Loss:1.2051 - Acc:47.36%\n",
      "284\n",
      "Train Epoch 1/50 - Loss:1.2067 - Acc:47.32%\n",
      "285\n",
      "Train Epoch 1/50 - Loss:1.2082 - Acc:47.29%\n",
      "286\n",
      "Train Epoch 1/50 - Loss:1.2096 - Acc:47.21%\n",
      "287\n",
      "Train Epoch 1/50 - Loss:1.2102 - Acc:47.18%\n",
      "288\n",
      "Train Epoch 1/50 - Loss:1.2093 - Acc:47.19%\n",
      "289\n",
      "Train Epoch 1/50 - Loss:1.2086 - Acc:47.20%\n",
      "290\n",
      "Train Epoch 1/50 - Loss:1.2078 - Acc:47.25%\n",
      "291\n",
      "Train Epoch 1/50 - Loss:1.2069 - Acc:47.35%\n",
      "292\n",
      "Train Epoch 1/50 - Loss:1.2069 - Acc:47.35%\n",
      "293\n",
      "Train Epoch 1/50 - Loss:1.2075 - Acc:47.32%\n",
      "294\n",
      "Train Epoch 1/50 - Loss:1.2079 - Acc:47.25%\n",
      "295\n",
      "Train Epoch 1/50 - Loss:1.2063 - Acc:47.42%\n",
      "296\n",
      "Train Epoch 1/50 - Loss:1.2067 - Acc:47.39%\n",
      "297\n",
      "Train Epoch 1/50 - Loss:1.2072 - Acc:47.36%\n",
      "298\n",
      "Train Epoch 1/50 - Loss:1.2062 - Acc:47.41%\n",
      "299\n",
      "Train Epoch 1/50 - Loss:1.2053 - Acc:47.50%\n",
      "300\n",
      "Train Epoch 1/50 - Loss:1.2051 - Acc:47.55%\n",
      "301\n",
      "Train Epoch 1/50 - Loss:1.2046 - Acc:47.52%\n",
      "302\n",
      "Train Epoch 1/50 - Loss:1.2043 - Acc:47.52%\n",
      "303\n",
      "Train Epoch 1/50 - Loss:1.2039 - Acc:47.57%\n",
      "304\n",
      "Train Epoch 1/50 - Loss:1.2034 - Acc:47.62%\n",
      "305\n",
      "Train Epoch 1/50 - Loss:1.2024 - Acc:47.63%\n",
      "306\n",
      "Train Epoch 1/50 - Loss:1.2013 - Acc:47.72%\n",
      "307\n",
      "Train Epoch 1/50 - Loss:1.1999 - Acc:47.85%\n",
      "308\n",
      "Train Epoch 1/50 - Loss:1.2001 - Acc:47.82%\n",
      "309\n",
      "Train Epoch 1/50 - Loss:1.2009 - Acc:47.70%\n",
      "310\n",
      "Train Epoch 1/50 - Loss:1.2013 - Acc:47.67%\n",
      "311\n",
      "Train Epoch 1/50 - Loss:1.2004 - Acc:47.72%\n",
      "312\n",
      "Train Epoch 1/50 - Loss:1.2005 - Acc:47.68%\n",
      "313\n",
      "Train Epoch 1/50 - Loss:1.1991 - Acc:47.81%\n",
      "314\n",
      "Train Epoch 1/50 - Loss:1.1990 - Acc:47.78%\n",
      "315\n",
      "Train Epoch 1/50 - Loss:1.1983 - Acc:47.82%\n",
      "316\n",
      "Train Epoch 1/50 - Loss:1.1981 - Acc:47.83%\n",
      "317\n",
      "Train Epoch 1/50 - Loss:1.1995 - Acc:47.76%\n",
      "318\n",
      "Train Epoch 1/50 - Loss:1.1994 - Acc:47.73%\n",
      "319\n",
      "Train Epoch 1/50 - Loss:1.1999 - Acc:47.73%\n",
      "320\n",
      "Train Epoch 1/50 - Loss:1.1988 - Acc:47.82%\n",
      "321\n",
      "Train Epoch 1/50 - Loss:1.1977 - Acc:47.94%\n",
      "322\n",
      "Train Epoch 1/50 - Loss:1.1978 - Acc:47.91%\n",
      "323\n",
      "Train Epoch 1/50 - Loss:1.1981 - Acc:47.96%\n",
      "324\n",
      "Train Epoch 1/50 - Loss:1.1971 - Acc:48.04%\n",
      "325\n",
      "Train Epoch 1/50 - Loss:1.1982 - Acc:47.97%\n",
      "326\n",
      "Train Epoch 1/50 - Loss:1.1986 - Acc:47.97%\n",
      "327\n",
      "Train Epoch 1/50 - Loss:1.1983 - Acc:47.98%\n",
      "328\n",
      "Train Epoch 1/50 - Loss:1.1982 - Acc:47.95%\n",
      "329\n",
      "Train Epoch 1/50 - Loss:1.1984 - Acc:47.95%\n",
      "330\n",
      "Train Epoch 1/50 - Loss:1.1976 - Acc:48.07%\n",
      "331\n",
      "Train Epoch 1/50 - Loss:1.1977 - Acc:48.00%\n",
      "332\n",
      "Train Epoch 1/50 - Loss:1.1981 - Acc:47.90%\n",
      "333\n",
      "Train Epoch 1/50 - Loss:1.1988 - Acc:47.75%\n",
      "334\n",
      "Train Epoch 1/50 - Loss:1.1984 - Acc:47.76%\n",
      "335\n",
      "Train Epoch 1/50 - Loss:1.1980 - Acc:47.77%\n",
      "336\n",
      "Train Epoch 1/50 - Loss:1.1982 - Acc:47.70%\n",
      "337\n",
      "Train Epoch 1/50 - Loss:1.1982 - Acc:47.71%\n",
      "338\n",
      "Train Epoch 1/50 - Loss:1.1978 - Acc:47.71%\n",
      "339\n",
      "Train Epoch 1/50 - Loss:1.1977 - Acc:47.68%\n",
      "340\n",
      "Train Epoch 1/50 - Loss:1.1971 - Acc:47.73%\n",
      "341\n",
      "Train Epoch 1/50 - Loss:1.1965 - Acc:47.73%\n",
      "342\n",
      "Train Epoch 1/50 - Loss:1.1963 - Acc:47.78%\n",
      "343\n",
      "Train Epoch 1/50 - Loss:1.1958 - Acc:47.82%\n",
      "344\n",
      "Train Epoch 1/50 - Loss:1.1950 - Acc:47.86%\n",
      "345\n",
      "Train Epoch 1/50 - Loss:1.1956 - Acc:47.83%\n",
      "346\n",
      "Train Epoch 1/50 - Loss:1.1946 - Acc:47.91%\n",
      "347\n",
      "Train Epoch 1/50 - Loss:1.1940 - Acc:47.95%\n",
      "348\n",
      "Train Epoch 1/50 - Loss:1.1934 - Acc:47.99%\n",
      "349\n",
      "Train Epoch 1/50 - Loss:1.1926 - Acc:48.07%\n",
      "350\n",
      "Train Epoch 1/50 - Loss:1.1930 - Acc:48.08%\n",
      "351\n",
      "Train Epoch 1/50 - Loss:1.1926 - Acc:48.12%\n",
      "352\n",
      "Train Epoch 1/50 - Loss:1.1938 - Acc:48.05%\n",
      "353\n",
      "Train Epoch 1/50 - Loss:1.1926 - Acc:48.13%\n",
      "354\n",
      "Train Epoch 1/50 - Loss:1.1924 - Acc:48.13%\n",
      "355\n",
      "Train Epoch 1/50 - Loss:1.1927 - Acc:48.07%\n",
      "356\n",
      "Train Epoch 1/50 - Loss:1.1924 - Acc:48.07%\n",
      "357\n",
      "Train Epoch 1/50 - Loss:1.1912 - Acc:48.18%\n",
      "358\n",
      "Train Epoch 1/50 - Loss:1.1900 - Acc:48.29%\n",
      "359\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_25428/815829495.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    168\u001b[0m     \u001b[0mbest_recall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m         \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m         \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'train_loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m         \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'train_accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_accuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_25428/815829495.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(net, optim)\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m         \u001b[0mtotal_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m         \u001b[0mtotal_correct\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[0mtotal_num\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from thop import profile, clever_format\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from dataset import SingleData\n",
    "from model import Model, set_bn_eval\n",
    "from utils import  LabelSmoothingCrossEntropyLoss, BatchHardTripletLoss, ImageReader, MPerClassSampler\n",
    "import numpy as np\n",
    "import os\n",
    "import math \n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def get_data_list(data_path, ratio=0.001):\n",
    "    img_list = []\n",
    "    for root, dirs, files in os.walk(data_path):\n",
    "        if files == []:\n",
    "            class_name = dirs\n",
    "        elif dirs == []:\n",
    "            for f in files:\n",
    "                img_path = os.path.join(root, f)\n",
    "                img_list.append(img_path)\n",
    "\n",
    "    np.random.seed(1)\n",
    "    train_img_list = np.random.choice(img_list, size=int(len(img_list)*(1-ratio)), replace=False)\n",
    "    #print(img_list, train_img_list)\n",
    "    eval_img_list = list(set(img_list) - set(train_img_list))\n",
    "    ########add\n",
    "    half=math.floor(len(eval_img_list)/2)\n",
    "    print(half)\n",
    "    eval_=eval_img_list[:half]\n",
    "    test_=eval_img_list[half:]\n",
    "    #######\n",
    "    #return class_name, train_img_list, eval_img_list \n",
    "    return class_name, train_img_list\n",
    "\n",
    "\n",
    "def train(net, optim):\n",
    "    net.train()\n",
    "    # fix bn on backbone network\n",
    "    net.apply(set_bn_eval)\n",
    "    # total_loss, total_correct, total_num, data_bar = 0, 0, 0, tqdm(train_data_loader)\n",
    "    print(\"start\")\n",
    "    total_loss, total_correct, total_num, data_bar = 0, 0, 0, enumerate(train_data_loader)\n",
    "    print(\"end\")\n",
    "    # for inputs, labels in data_bar:\n",
    "    for batch_idx, (inputs, labels, _) in data_bar:\n",
    "        print(batch_idx)\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        features, classes = net(inputs)\n",
    "        class_loss = class_criterion(classes, labels)\n",
    "        feature_loss = feature_criterion(features, labels)\n",
    "        loss = class_loss + feature_loss\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        pred = torch.argmax(classes, dim=-1)\n",
    "        total_loss += loss.item() * inputs.size(0)\n",
    "        total_correct += torch.sum(pred == labels).item()\n",
    "        total_num += inputs.size(0)\n",
    "        print('Train Epoch {}/{} - Loss:{:.4f} - Acc:{:.2f}%'.format(epoch, num_epochs, total_loss / total_num, total_correct / total_num * 100))\n",
    "        # data_bar.set_description('Train Epoch {}/{} - Loss:{:.4f} - Acc:{:.2f}%'\n",
    "        #                          .format(epoch, num_epochs, total_loss / total_num, total_correct / total_num * 100))\n",
    "\n",
    "    return total_loss / total_num, total_correct / total_num * 100\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    \n",
    "    # parser = argparse.ArgumentParser(description='Train CGD')\n",
    "    # parser.add_argument('--data_path', default='/home/data', type=str, help='datasets path')\n",
    "    # parser.add_argument('--data_name', default='car', type=str, choices=['car', 'cub', 'sop', 'isc'],\n",
    "    #                     help='dataset name')\n",
    "    # parser.add_argument('--crop_type', default='uncropped', type=str, choices=['uncropped', 'cropped'],\n",
    "    #                     help='crop data or not, it only works for car or cub dataset')\n",
    "    # parser.add_argument('--backbone_type', default='resnet50', type=str, choices=['resnet50', 'resnext50'],\n",
    "    #                     help='backbone network type')\n",
    "    # parser.add_argument('--gd_config', default='SG', type=str,\n",
    "    #                     choices=['S', 'M', 'G', 'SM', 'MS', 'SG', 'GS', 'MG', 'GM', 'SMG', 'MSG', 'GSM'],\n",
    "    #                     help='global descriptors config')\n",
    "    # parser.add_argument('--feature_dim', default=1536, type=int, help='feature dim')\n",
    "    # parser.add_argument('--smoothing', default=0.1, type=float, help='smoothing value for label smoothing')\n",
    "    # parser.add_argument('--temperature', default=0.5, type=float,\n",
    "    #                     help='temperature scaling used in softmax cross-entropy loss')\n",
    "    # parser.add_argument('--margin', default=0.1, type=float, help='margin of m for triplet loss')\n",
    "    # parser.add_argument('--recalls', default='1,2,4,8', type=str, help='selected recall')\n",
    "    # parser.add_argument('--batch_size', default=128, type=int, help='train batch size')\n",
    "    # parser.add_argument('--num_epochs', default=20, type=int, help='train epoch number')\n",
    "\n",
    "    # opt = parser.parse_args()\n",
    "    # args parse\n",
    "    data_path=\"D:\\\\original_images_5\\\\few-person\\\\for colab\\\\train\"\n",
    "    # data_path=\"D:\\\\models\\\\CGD-master\\\\dataset\\\\img\"\n",
    "    test_path=\"D:\\\\original_images_5\\\\few-person\\\\for colab\\\\val\"\n",
    "    data_name=\"CRC\"\n",
    "    crop_type=\"uncropped\"\n",
    "    backbone_type=\"resnet50\"\n",
    "    gd_config=\"MG\"\n",
    "    feature_dim=1536\n",
    "    smoothing=0.1\n",
    "    temperature=0.5\n",
    "    margin=0.1\n",
    "    tempRecall='1,2,4,8'\n",
    "    recalls=[int(k) for k in tempRecall.split(',')]\n",
    "    batch_size=8\n",
    "    num_epochs=50\n",
    "    \n",
    "    class_name, train_img_list = get_data_list(data_path)\n",
    "    class_test_name, test_img_list = get_data_list(test_path)\n",
    "    \n",
    "    train_transform = transforms.Compose([ \n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "    \n",
    "    \n",
    "    # data_path, data_name, crop_type, backbone_type = opt.data_path, opt.data_name, opt.crop_type, opt.backbone_type\n",
    "    # gd_config, feature_dim, smoothing, temperature = opt.gd_config, opt.feature_dim, opt.smoothing, opt.temperature\n",
    "    # margin, recalls, batch_size = opt.margin, [int(k) for k in opt.recalls.split(',')], opt.batch_size\n",
    "    # num_epochs = opt.num_epochs\n",
    "    save_name_pre = '{}_{}_{}_{}_{}_{}_{}_{}_{}_gs_pretrian'.format(data_name, crop_type, backbone_type, gd_config, feature_dim,\n",
    "                                                        smoothing, temperature, margin, batch_size)\n",
    "\n",
    "    results = {'train_loss': [], 'train_accuracy': []}\n",
    "#     resultsWithC = {'train_loss': [], 'train_accuracy': []}\n",
    "    \n",
    "    for recall_id in recalls:\n",
    "        results['test_recall@{}'.format(recall_id)] = []\n",
    "#         resultsWithC['test_recall@{}'.format(recall_id)] = []\n",
    "        \n",
    "\n",
    "    train_data_set=SingleData(class_name, train_img_list, train_transform)\n",
    "    # dataset loader\n",
    "    # train_data_set = ImageReader(data_path, data_name, 'train', crop_type)\n",
    "    # train_sample = MPerClassSampler(train_data_set.labels, batch_size)\n",
    "    train_sample = MPerClassSampler(train_data_set.label_list, batch_size)\n",
    "    train_data_loader = DataLoader(train_data_set, batch_sampler=train_sample, num_workers=8)\n",
    "    # test_data_set = ImageReader(data_path, data_name, 'query' if data_name == 'isc' else 'test', crop_type)\n",
    "    test_data_set = SingleData(class_test_name, test_img_list, train_transform)\n",
    "    test_data_loader = DataLoader(test_data_set, batch_size, shuffle=False, num_workers=8)\n",
    "    eval_dict = {'test': {'data_loader': test_data_loader}}\n",
    "    if data_name == 'isc':\n",
    "        gallery_data_set = ImageReader(data_path, data_name, 'gallery', crop_type)\n",
    "        gallery_data_loader = DataLoader(gallery_data_set, batch_size, shuffle=False, num_workers=8)\n",
    "        eval_dict['gallery'] = {'data_loader': gallery_data_loader}\n",
    "\n",
    "    # model setup, model profile, optimizer config and loss definition\n",
    "    model = Model(backbone_type, gd_config, feature_dim, num_classes=3).cuda()\n",
    "    # model.load_state_dict(torch.load('D:\\\\models\\\\CGD-master_2\\\\CGD-master\\\\isc_uncropped_resnet50_GS_1536_0.1_0.5_0.1_128_model.pth'))\n",
    "\n",
    "    flops, params = profile(model, inputs=(torch.randn(1, 3, 224, 224).cuda(),))\n",
    "    flops, params = clever_format([flops, params])\n",
    "    print('# Model Params: {} FLOPs: {}'.format(params, flops))\n",
    "    optimizer = Adam(model.parameters(), lr=1e-4)\n",
    "    lr_scheduler = MultiStepLR(optimizer, milestones=[int(0.6 * num_epochs), int(0.8 * num_epochs)], gamma=0.1)\n",
    "    class_criterion = LabelSmoothingCrossEntropyLoss(smoothing=smoothing, temperature=temperature)\n",
    "    feature_criterion = BatchHardTripletLoss(margin=margin)\n",
    "\n",
    "    best_recall = 0.0\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train_loss, train_accuracy = train(model, optimizer)\n",
    "        results['train_loss'].append(train_loss)\n",
    "        results['train_accuracy'].append(train_accuracy)\n",
    "#         resultsWithC['train_loss'].append(train_loss)\n",
    "#         resultsWithC['train_accuracy'].append(train_accuracy)\n",
    "        rank = test(model, recalls)\n",
    "#         rank2 = testWithTask(model, recalls)\n",
    "        \n",
    "        lr_scheduler.step()\n",
    "\n",
    "        # save statistics\n",
    "        data_frame = pd.DataFrame(data=results, index=range(1, epoch + 1))\n",
    "#         data_frame_with_condition = pd.DataFrame(data=resultsWithC, index=range(1, epoch + 1))\n",
    "        \n",
    "        data_frame.to_csv('results/{}_statistics.csv'.format(save_name_pre), index_label='epoch')\n",
    "#         data_frame_with_condition.to_csv('results/{}_statistics_with_condition.csv'.format(save_name_pre), index_label='epoch')\n",
    "        \n",
    "        # save database and model\n",
    "        data_base = {}\n",
    "        if rank > best_recall:\n",
    "            best_recall = rank\n",
    "            data_base['test_images'] = test_data_set.img_list\n",
    "            data_base['test_labels'] = test_data_set.label_list\n",
    "            data_base['test_features'] = eval_dict['test']['features']\n",
    "            if data_name == 'isc':\n",
    "                data_base['gallery_images'] = gallery_data_set.img_list\n",
    "                data_base['gallery_labels'] = gallery_data_set.label_list\n",
    "                data_base['gallery_features'] = eval_dict['gallery']['features']\n",
    "            torch.save(model.state_dict(), 'results/{}_{}_model.pth'.format(save_name_pre,epoch))\n",
    "            torch.save(data_base, 'results/{}_{}_data_base.pth'.format(save_name_pre,epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92f0049e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_bn() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "\u001b[91m[WARN] Cannot find rule for <class 'torch.nn.modules.container.Sequential'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
      "\u001b[91m[WARN] Cannot find rule for <class 'resnet.Bottleneck'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
      "\u001b[91m[WARN] Cannot find rule for <class 'model.GlobalDescriptor'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
      "\u001b[91m[WARN] Cannot find rule for <class 'torch.nn.modules.container.ModuleList'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "\u001b[91m[WARN] Cannot find rule for <class 'model.L2Norm'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
      "[INFO] Register count_bn() for <class 'torch.nn.modules.batchnorm.BatchNorm1d'>.\n",
      "\u001b[91m[WARN] Cannot find rule for <class 'model.Model'>. Treat it as zero Macs and zero Params.\u001b[00m\n",
      "# Model Params: 26.66M FLOPs: 10.64G\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(backbone_type, gd_config, feature_dim, num_classes=3).cuda()\n",
    "flops, params = profile(model, inputs=(torch.randn(1, 3, 224, 224).cuda(),))\n",
    "flops, params = clever_format([flops, params])\n",
    "print('# Model Params: {} FLOPs: {}'.format(params, flops))\n",
    "optimizer = Adam(model.parameters(), lr=1e-4)\n",
    "lr_scheduler = MultiStepLR(optimizer, milestones=[int(0.6 * num_epochs), int(0.8 * num_epochs)], gamma=0.1)\n",
    "class_criterion = LabelSmoothingCrossEntropyLoss(smoothing=smoothing, temperature=temperature)\n",
    "feature_criterion = BatchHardTripletLoss(margin=margin)   \n",
    "model.load_state_dict(torch.load('D:\\\\models\\\\backup2\\\\CGD-master\\\\results\\\\CRC-MG\\\\CRC_uncropped_resnet50_MG_1536_0.1_0.5_0.1_8_gs_pretrian_4_model.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de9e2782",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsWithC={}\n",
    "for recall_id in recalls:\n",
    "    resultsWithC['test_recall@{}'.format(recall_id)] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "747e6eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing test data\n",
      "Test Epoch 50/50 R@1:32.15% R@2:52.50% R@4:78.09% R@8:94.77% \n",
      "{'test_recall@1': [], 'test_recall@2': [], 'test_recall@4': [], 'test_recall@8': []}\n"
     ]
    }
   ],
   "source": [
    "rank2 = test(model, recalls)\n",
    "print(resultsWithC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5177536",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testWithTask(net, recall_ids):\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        # obtain feature vectors for all data\n",
    "        for key in eval_dict.keys():\n",
    "            eval_dict[key]['features'] = []\n",
    "            imgList=[]\n",
    "            # for inputs, labels in tqdm(eval_dict[key]['data_loader'], desc='processing {} data'.format(key)):\n",
    "            print('processing {} data'.format(key))\n",
    "            for batch_idx, (inputs, labels, imgs) in enumerate(eval_dict[key]['data_loader']):\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "                features, classes = net(inputs)\n",
    "                eval_dict[key]['features'].append(features)\n",
    "                imgList+=imgs\n",
    "            eval_dict[key]['features'] = torch.cat(eval_dict[key]['features'], dim=0)\n",
    "\n",
    "        # compute recall metric\n",
    "        if data_name == 'isc':\n",
    "            acc_list = recallWithTask(eval_dict['test']['features'], test_data_set.label_list, recall_ids,imgList,\n",
    "                              eval_dict['gallery']['features'], gallery_data_set.label_list)\n",
    "        else:\n",
    "            acc_list = recallWithTask(eval_dict['test']['features'], test_data_set.label_list, recall_ids,imgList)\n",
    "    \n",
    "    desc = 'Test Epoch {}/{} '.format(epoch, num_epochs)\n",
    "    for index, rank_id in enumerate(recall_ids):\n",
    "        desc += 'R@{}:{:.2f}%'.format(rank_id, acc_list[index] * 100)\n",
    "        resultsWithC['test_recall@{}'.format(rank_id)].append(acc_list[index] * 100)\n",
    "    print(desc)\n",
    "    return acc_list[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddf00d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recallWithTask(feature_vectors, feature_labels, rank,test_img_list, gallery_vectors=None, gallery_labels=None):\n",
    "    num_features = len(feature_labels)\n",
    "    feature_labels = torch.tensor(feature_labels, device=feature_vectors.device)\n",
    "    gallery_vectors = feature_vectors if gallery_vectors is None else gallery_vectors\n",
    "\n",
    "    dist_matrix = torch.cdist(feature_vectors.unsqueeze(0), gallery_vectors.unsqueeze(0)).squeeze(0)\n",
    "    \n",
    "    for i in range(len(test_img_list)):\n",
    "        for j in range(len(test_img_list)):\n",
    "            index1=test_img_list[i].rfind('\\\\')\n",
    "            index2=test_img_list[j].rfind('\\\\')\n",
    "            if test_img_list[i][index1+4:index1+7] == test_img_list[j][index2+4:index2+7] and i!=j :\n",
    "                dist_matrix[i][j]=float('inf')\n",
    "    \n",
    "    \n",
    "    if gallery_labels is None:\n",
    "        dist_matrix.fill_diagonal_(float('inf'))\n",
    "        gallery_labels = feature_labels\n",
    "    else:\n",
    "        gallery_labels = torch.tensor(gallery_labels, device=feature_vectors.device)\n",
    "\n",
    "    idx = dist_matrix.topk(k=rank[-1], dim=-1, largest=False)[1]\n",
    "    acc_list = []\n",
    "    for r in rank:\n",
    "        correct = (gallery_labels[idx[:, 0:r]] == feature_labels.unsqueeze(dim=-1)).any(dim=-1).float()\n",
    "        acc_list.append((torch.sum(correct) / num_features).item())\n",
    "    return acc_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "754ee0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "['s1', 's2', 's3']\n"
     ]
    }
   ],
   "source": [
    "test_path=\"D:\\\\original_images_5\\\\few-person\\\\for colab\\\\different test\"\n",
    "class_test_name, test_img_list = get_data_list(test_path)\n",
    "\n",
    "\n",
    "test_data_set = SingleData(class_test_name, test_img_list, train_transform)\n",
    "test_data_loader = DataLoader(test_data_set, batch_size, shuffle=False, num_workers=8)\n",
    "eval_dict = {'test': {'data_loader': test_data_loader}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "efba2eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing test data\n",
      "Test Epoch 50/50 R@1:26.98%R@2:48.02%R@4:73.63%R@8:92.07%\n",
      "{'test_recall@1': [26.9817054271698], 'test_recall@2': [48.01829159259796], 'test_recall@4': [73.62804412841797], 'test_recall@8': [92.07316637039185]}\n"
     ]
    }
   ],
   "source": [
    "resultsWithC={}\n",
    "for recall_id in recalls:\n",
    "    resultsWithC['test_recall@{}'.format(recall_id)] = []\n",
    "rank2 = testWithTask(model, recalls)\n",
    "print(resultsWithC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fc971c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

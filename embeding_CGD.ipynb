{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from thop import profile, clever_format\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from dataset import SingleData\n",
    "from model import Model, set_bn_eval\n",
    "from utils import  LabelSmoothingCrossEntropyLoss, BatchHardTripletLoss, ImageReader, MPerClassSampler\n",
    "import numpy as np\n",
    "import os\n",
    "import math \n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def get_data_list(data_path, ratio=0.001):\n",
    "    img_list = []\n",
    "    for root, dirs, files in os.walk(data_path):\n",
    "        if files == []:\n",
    "            class_name = dirs\n",
    "        elif dirs == []:\n",
    "            for f in files:\n",
    "                img_path = os.path.join(root, f)\n",
    "                img_list.append(img_path)\n",
    "\n",
    "    np.random.seed(1)\n",
    "    train_img_list = np.random.choice(img_list, size=int(len(img_list)*(1-ratio)), replace=False)\n",
    "    #print(img_list, train_img_list)\n",
    "    eval_img_list = list(set(img_list) - set(train_img_list))\n",
    "    ########add\n",
    "    half=math.floor(len(eval_img_list)/2)\n",
    "    print(half)\n",
    "    eval_=eval_img_list[:half]\n",
    "    test_=eval_img_list[half:]\n",
    "    #######\n",
    "    #return class_name, train_img_list, eval_img_list \n",
    "    return class_name, train_img_list,img_list\n",
    "\n",
    "\n",
    "def train(net, optim):\n",
    "    net.train()\n",
    "    # fix bn on backbone network\n",
    "    net.apply(set_bn_eval)\n",
    "    # total_loss, total_correct, total_num, data_bar = 0, 0, 0, tqdm(train_data_loader)\n",
    "    print(\"start\")\n",
    "    total_loss, total_correct, total_num, data_bar = 0, 0, 0, enumerate(train_data_loader)\n",
    "    print(\"end\")\n",
    "    # for inputs, labels in data_bar:\n",
    "    for batch_idx, (inputs, labels, _) in data_bar:\n",
    "        print(batch_idx)\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        features, classes = net(inputs)\n",
    "        class_loss = class_criterion(classes, labels)\n",
    "        feature_loss = feature_criterion(features, labels)\n",
    "        loss = class_loss + feature_loss\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        pred = torch.argmax(classes, dim=-1)\n",
    "        total_loss += loss.item() * inputs.size(0)\n",
    "        total_correct += torch.sum(pred == labels).item()\n",
    "        total_num += inputs.size(0)\n",
    "        print('Train Epoch {}/{} - Loss:{:.4f} - Acc:{:.2f}%'.format(epoch, num_epochs, total_loss / total_num, total_correct / total_num * 100))\n",
    "        # data_bar.set_description('Train Epoch {}/{} - Loss:{:.4f} - Acc:{:.2f}%'\n",
    "        #                          .format(epoch, num_epochs, total_loss / total_num, total_correct / total_num * 100))\n",
    "\n",
    "    return total_loss / total_num, total_correct / total_num * 100\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    \n",
    "    # parser = argparse.ArgumentParser(description='Train CGD')\n",
    "    # parser.add_argument('--data_path', default='/home/data', type=str, help='datasets path')\n",
    "    # parser.add_argument('--data_name', default='car', type=str, choices=['car', 'cub', 'sop', 'isc'],\n",
    "    #                     help='dataset name')\n",
    "    # parser.add_argument('--crop_type', default='uncropped', type=str, choices=['uncropped', 'cropped'],\n",
    "    #                     help='crop data or not, it only works for car or cub dataset')\n",
    "    # parser.add_argument('--backbone_type', default='resnet50', type=str, choices=['resnet50', 'resnext50'],\n",
    "    #                     help='backbone network type')\n",
    "    # parser.add_argument('--gd_config', default='SG', type=str,\n",
    "    #                     choices=['S', 'M', 'G', 'SM', 'MS', 'SG', 'GS', 'MG', 'GM', 'SMG', 'MSG', 'GSM'],\n",
    "    #                     help='global descriptors config')\n",
    "    # parser.add_argument('--feature_dim', default=1536, type=int, help='feature dim')\n",
    "    # parser.add_argument('--smoothing', default=0.1, type=float, help='smoothing value for label smoothing')\n",
    "    # parser.add_argument('--temperature', default=0.5, type=float,\n",
    "    #                     help='temperature scaling used in softmax cross-entropy loss')\n",
    "    # parser.add_argument('--margin', default=0.1, type=float, help='margin of m for triplet loss')\n",
    "    # parser.add_argument('--recalls', default='1,2,4,8', type=str, help='selected recall')\n",
    "    # parser.add_argument('--batch_size', default=128, type=int, help='train batch size')\n",
    "    # parser.add_argument('--num_epochs', default=20, type=int, help='train epoch number')\n",
    "\n",
    "    # opt = parser.parse_args()\n",
    "    # args parse\n",
    "    data_path=\"C:/Users/user01/Documents/dataset/CRC-test\"\n",
    "    # data_path=\"D:\\\\models\\\\CGD-master\\\\dataset\\\\img\"\n",
    "    test_path=\"C:/Users/user01/Documents/dataset/CRC-test\"\n",
    "    data_name=\"CRC\"\n",
    "    crop_type=\"uncropped\"\n",
    "    backbone_type=\"resnet50\"\n",
    "    gd_config=\"SM\"\n",
    "    feature_dim=1536\n",
    "    smoothing=0.1\n",
    "    temperature=0.5\n",
    "    margin=0.1\n",
    "    tempRecall='1,2,4,8'\n",
    "    recalls=[int(k) for k in tempRecall.split(',')]\n",
    "    batch_size=8\n",
    "    num_epochs=0\n",
    "    \n",
    "    class_name, train_img_list,_ = get_data_list(data_path)\n",
    "    class_test_name, test_img_list = get_data_list(test_path)\n",
    "    \n",
    "    train_transform = transforms.Compose([ \n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomCrop(96),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "    \n",
    "    \n",
    "    # data_path, data_name, crop_type, backbone_type = opt.data_path, opt.data_name, opt.crop_type, opt.backbone_type\n",
    "    # gd_config, feature_dim, smoothing, temperature = opt.gd_config, opt.feature_dim, opt.smoothing, opt.temperature\n",
    "    # margin, recalls, batch_size = opt.margin, [int(k) for k in opt.recalls.split(',')], opt.batch_size\n",
    "    # num_epochs = opt.num_epochs\n",
    "    save_name_pre = '{}_{}_{}_{}_{}_{}_{}_{}_{}_sm_for_self_sup'.format(data_name, crop_type, backbone_type, gd_config, feature_dim,\n",
    "                                                        smoothing, temperature, margin, batch_size)\n",
    "\n",
    "    results = {'train_loss': [], 'train_accuracy': []}\n",
    "#     resultsWithC = {'train_loss': [], 'train_accuracy': []}\n",
    "    \n",
    "    for recall_id in recalls:\n",
    "        results['test_recall@{}'.format(recall_id)] = []\n",
    "#         resultsWithC['test_recall@{}'.format(recall_id)] = []\n",
    "        \n",
    "\n",
    "    train_data_set=SingleData(class_name, train_img_list, train_transform)\n",
    "    # dataset loader\n",
    "    # train_data_set = ImageReader(data_path, data_name, 'train', crop_type)\n",
    "    # train_sample = MPerClassSampler(train_data_set.labels, batch_size)\n",
    "    train_sample = MPerClassSampler(train_data_set.label_list, batch_size)\n",
    "    train_data_loader = DataLoader(train_data_set, batch_sampler=train_sample, num_workers=8)\n",
    "    # test_data_set = ImageReader(data_path, data_name, 'query' if data_name == 'isc' else 'test', crop_type)\n",
    "    test_data_set = SingleData(class_test_name, test_img_list, train_transform)\n",
    "    test_data_loader = DataLoader(test_data_set, batch_size, shuffle=False, num_workers=8)\n",
    "    eval_dict = {'test': {'data_loader': test_data_loader}}\n",
    "    if data_name == 'isc':\n",
    "        gallery_data_set = ImageReader(data_path, data_name, 'gallery', crop_type)\n",
    "        gallery_data_loader = DataLoader(gallery_data_set, batch_size, shuffle=False, num_workers=8)\n",
    "        eval_dict['gallery'] = {'data_loader': gallery_data_loader}\n",
    "\n",
    "    # model setup, model profile, optimizer config and loss definition\n",
    "    model = Model(backbone_type, gd_config, feature_dim, num_classes=3).cuda()\n",
    "    # model.load_state_dict(torch.load('D:\\\\models\\\\CGD-master_2\\\\CGD-master\\\\isc_uncropped_resnet50_GS_1536_0.1_0.5_0.1_128_model.pth'))\n",
    "\n",
    "    flops, params = profile(model, inputs=(torch.randn(1, 3, 96, 96).cuda(),))\n",
    "    flops, params = clever_format([flops, params])\n",
    "    print('# Model Params: {} FLOPs: {}'.format(params, flops))\n",
    "    optimizer = Adam(model.parameters(), lr=1e-4)\n",
    "    lr_scheduler = MultiStepLR(optimizer, milestones=[int(0.6 * num_epochs), int(0.8 * num_epochs)], gamma=0.1)\n",
    "    class_criterion = LabelSmoothingCrossEntropyLoss(smoothing=smoothing, temperature=temperature)\n",
    "    feature_criterion = BatchHardTripletLoss(margin=margin)\n",
    "\n",
    "#     best_recall = 0.0\n",
    "#     for epoch in range(1, num_epochs + 1):\n",
    "#         train_loss, train_accuracy = train(model, optimizer)\n",
    "#         results['train_loss'].append(train_loss)\n",
    "#         results['train_accuracy'].append(train_accuracy)\n",
    "# #         resultsWithC['train_loss'].append(train_loss)\n",
    "# #         resultsWithC['train_accuracy'].append(train_accuracy)\n",
    "#         rank = test(model, recalls)\n",
    "# #         rank2 = testWithTask(model, recalls)\n",
    "        \n",
    "#         lr_scheduler.step()\n",
    "\n",
    "#         # save statistics\n",
    "#         data_frame = pd.DataFrame(data=results, index=range(1, epoch + 1))\n",
    "# #         data_frame_with_condition = pd.DataFrame(data=resultsWithC, index=range(1, epoch + 1))\n",
    "        \n",
    "#         data_frame.to_csv('results/{}_statistics.csv'.format(save_name_pre), index_label='epoch')\n",
    "# #         data_frame_with_condition.to_csv('results/{}_statistics_with_condition.csv'.format(save_name_pre), index_label='epoch')\n",
    "        \n",
    "#         # save database and model\n",
    "#         data_base = {}\n",
    "#         if rank > best_recall:\n",
    "#             best_recall = rank\n",
    "#             data_base['test_images'] = test_data_set.img_list\n",
    "#             data_base['test_labels'] = test_data_set.label_list\n",
    "#             data_base['test_features'] = eval_dict['test']['features']\n",
    "#             if data_name == 'isc':\n",
    "#                 data_base['gallery_images'] = gallery_data_set.img_list\n",
    "#                 data_base['gallery_labels'] = gallery_data_set.label_list\n",
    "#                 data_base['gallery_features'] = eval_dict['gallery']['features']\n",
    "#             torch.save(model.state_dict(), 'results/{}_{}_model.pth'.format(save_name_pre,epoch))\n",
    "#             torch.save(data_base, 'results/{}_{}_data_base.pth'.format(save_name_pre,epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('C:/Users/user01/Documents/code/CGD-master/results/CRC_uncropped_resnet50_SM_1536_0.1_0.5_0.1_8_gs_pretrian_29_model.pth'))\n",
    "import torchvision\n",
    "dataset=torchvision.datasets.ImageFolder('C:/Users/user01/Documents/dataset/CRC-test',train_transform)\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "test_img_list=[]\n",
    "for i in range(len(dataset.imgs)):\n",
    "    test_img_list.append(dataset.imgs[i][0].replace('\\\\','/'))\n",
    "torch.save(test_img_list,'test_img_list.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net=model.eval()\n",
    "all_embeddings = []\n",
    "i=0\n",
    "with torch.no_grad():\n",
    "        for inputs, labels in test_dataloader:\n",
    "                # print(batch_idx)\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "                features, classes = net(inputs)\n",
    "                all_embeddings.append(features.cpu())\n",
    "                # break\n",
    "                # i+=1\n",
    "                # if i ==3:\n",
    "                #         break\n",
    "        all_embeddings = torch.cat(all_embeddings, dim=0)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
